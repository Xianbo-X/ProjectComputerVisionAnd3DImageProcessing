{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yju6GyB1GWhm"
      },
      "source": [
        "[File colab link](https://colab.research.google.com/drive/1bXbzJgUoGeIM_7EMgnh6p-UsvAkRY8Tj#scrollTo=31EWlxEpDA_b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lB4HLpu7CIiT"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import traceback"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GGNfcuXTCoAr"
      },
      "outputs": [],
      "source": [
        "DEBUG=True\n",
        "batch_size=128\n",
        "shuffle_flag=False if DEBUG else False\n",
        "train_data=torchvision.datasets.MNIST(\"./data\",train=True,download=True,transform=torchvision.transforms.ToTensor())\n",
        "test_data=torchvision.datasets.MNIST(\"./data\",train=False,download=True,transform=torchvision.transforms.ToTensor())\n",
        "train_loader=DataLoader(train_data,batch_size=batch_size,shuffle=shuffle_flag)\n",
        "test_loader=DataLoader(test_data,batch_size=batch_size,shuffle=shuffle_flag)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "31EWlxEpDA_b",
        "outputId": "50d94131-fb5d-4653-fd6e-78157ab50c79"
      },
      "outputs": [],
      "source": [
        "plt.imshow(train_data.data[0].detach().numpy(),cmap=\"gray\")\n",
        "plt.title(\"digit {}\".format(train_data.targets[0].detach().numpy()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6_YBVYvPbAj"
      },
      "source": [
        "## Create network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "nKYuvz9tPcnZ"
      },
      "outputs": [],
      "source": [
        "class ConvNet(nn.Module):\n",
        "  def __init__(self,in_feature_2d,out_feature):\n",
        "    self.in_feature_2d=in_feature_2d\n",
        "    super(ConvNet,self).__init__()\n",
        "    self.layer_2d=[]\n",
        "    self. _set_network(in_feature_2d,out_feature) # Internal protected method\n",
        "\n",
        "  def _set_network(self,in_feature_2d,out_feature):\n",
        "    self.conv1=nn.Conv2d(in_channels=1,out_channels=16,kernel_size=(5,5),stride=1,padding=\"same\")\n",
        "    self.layer_2d.append(self.conv1)\n",
        "\n",
        "    self.act1=nn.ReLU()\n",
        "    self.layer_2d.append(self.act1)\n",
        "\n",
        "    self.pooling1=nn.MaxPool2d(kernel_size=(2,2),stride=2)\n",
        "    self.layer_2d.append(self.pooling1)\n",
        "\n",
        "    self.conv2=nn.Conv2d(in_channels=16,out_channels=32,kernel_size=(3,3),padding=\"same\")\n",
        "    self.layer_2d.append(self.conv2)\n",
        "\n",
        "    self.act2=nn.ReLU()\n",
        "    self.layer_2d.append(self.act2)\n",
        "\n",
        "    self.pooling2=nn.MaxPool2d(kernel_size=(2,2),stride=2)\n",
        "    self.layer_2d.append(self.pooling2)\n",
        "\n",
        "    self.conv3=nn.Conv2d(in_channels=32,out_channels=32,kernel_size=(3,3),padding=\"same\")\n",
        "    self.layer_2d.append(self.conv3)\n",
        "\n",
        "    self.act3=nn.ReLU()\n",
        "    self.layer_2d.append(self.act3)\n",
        "\n",
        "    self.pooling3=nn.MaxPool2d(kernel_size=(7,7)) # global max pooling with pooling size of the input\n",
        "    self.layer_2d.append(self.pooling3)\n",
        "\n",
        "    self.featureFC=nn.Linear(in_features=32,out_features=2,bias=True)\n",
        "    self.outputFC=nn.Linear(in_features=2,out_features=out_feature,bias=True)\n",
        "  \n",
        "  def forward(self,x):\n",
        "    feature=self.feature_extraction(x)\n",
        "    return self.outputFC(feature)\n",
        "  \n",
        "  def feature_extraction(self,x):\n",
        "    input=x\n",
        "    for layer in self.layer_2d:\n",
        "      output=layer(input)\n",
        "      input=output\n",
        "    output=self.featureFC(output.squeeze())\n",
        "    return output\n",
        "  \n",
        "  def predict_prob(self,x):\n",
        "    output=self.forward(x)\n",
        "    return output.softmax(dim=1) # apply softmax to each row, each batch\n",
        "  \n",
        "  def predict(self,x):\n",
        "    prob=self.predict_prob(x)\n",
        "    return prob.argmax(dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "qUyr-RfnrtG9"
      },
      "outputs": [],
      "source": [
        "input_size_2d=np.array(train_data.data[0].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "-M06ac4xrRvV"
      },
      "outputs": [],
      "source": [
        "model=ConvNet(input_size_2d,10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "6y-DVlvEs1iX"
      },
      "outputs": [],
      "source": [
        "lr=0.1\n",
        "EPOCHES=100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "XzLz7sxWsQO-"
      },
      "outputs": [],
      "source": [
        "optimizer=optim.SGD(model.parameters(),lr=lr)\n",
        "loss_func=nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "6LW1pp6Ys5Fe"
      },
      "outputs": [],
      "source": [
        "def train( model, train_loader, test_loader, optimizer, loss_func, EPOCHES, device=torch.device(\"cpu\")):\n",
        "    model.to(device)\n",
        "    model.train()\n",
        "    train_loss_recorder = []\n",
        "    lr_recorder = []\n",
        "    for epoch in range(EPOCHES):\n",
        "        avg_loss = 0\n",
        "        for batch_idx, (data, target) in enumerate(train_loader):\n",
        "            data = data.to(device)\n",
        "            target = target.to(device)\n",
        "\n",
        "            optimizer.zero_grad()  # clear all grad to avoid cumulation of grad\n",
        "            output = model(data)\n",
        "            loss_val = loss_func(output, target)\n",
        "            avg_loss += loss_val.item()\n",
        "            loss_val.backward()\n",
        "\n",
        "            # if batch_idx % 100 == 0:\n",
        "            #     print(\n",
        "            #         \"Train Epoch:{}/{} [{}/{} ({:.0f}%)] \\t Loss: {:.6f}\\r\".format(\n",
        "            #             epoch + 1,\n",
        "            #             EPOCHES,\n",
        "            #             batch_idx * len(data),\n",
        "            #             len(train_loader.dataset),\n",
        "            #             100 * (batch_idx / len(train_loader)),\n",
        "            #             loss_val.item(),\n",
        "            #         ),\n",
        "            #         end=\"\",\n",
        "            #     )\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        avg_loss = avg_loss / (len(train_loader))\n",
        "        train_loss_recorder.append([epoch, avg_loss])\n",
        "        lr_recorder.append([epoch, optimizer.param_groups[0][\"lr\"]])\n",
        "        if epoch % 20 ==0:\n",
        "            print(\n",
        "                \"Train Epoch:{}/{} \\t Average Loss: {:.6f}\\r\".format(\n",
        "                    epoch+1, EPOCHES, avg_loss\n",
        "                )\n",
        "            )\n",
        "            torch.save(model.state_dict(), \"model_epoch_{}.pth\".format(epoch))\n",
        "            torch.save(optimizer.state_dict(), \"optimizer_epoch_{}.pth\".format(epoch))\n",
        "    return {\"loss\": {\"train_loss\": train_loss_recorder}, \"lr\": lr_recorder}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uO5_9-_fuNfn",
        "outputId": "438cc891-67cb-42d0-8b24-74a68740442b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch:1/2 \t Average Loss: 2.418369\n"
          ]
        }
      ],
      "source": [
        "recorded_message=train(model,train_loader,test_loader,optimizer,loss_func,EPOCHES)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "pSKwUIMg4HvJ"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'train_loss': [[0, 2.4183692723703283], [1, 2.4150800755791573]]}"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "recorded_message[\"loss\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TestTools():\n",
        "    def __init__(self) -> None:\n",
        "        pass\n",
        "def accuracy(model,dataset):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        result=model.predict(dataset.data.reshape(-1,1,28,28).float())\n",
        "    torch.sum(result.argmax(dim=1)==train_data.targets).item()/len(train_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "  result=model.predict_prob(train_data.data.reshape(-1,1,28,28).float())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.10441666666666667"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "ReID-Mnist.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
